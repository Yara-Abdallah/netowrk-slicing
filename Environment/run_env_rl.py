from .utils.imports import *
from .utils.period import Period


class Environment:
    size = 0
    data = {}
    Grids = {}
    steps = 0
    average_qvalue_centralize = []
    # Initialize previous_steps variable
    previous_steps = 0
    frame_rate_for_sending_requests = 1
    previous_steps_sending = 0
    previous_period = 0
    snapshot_time = 5
    previous_steps_centralize = 0
    previous_steps_centralize_action = 0
    previouse_steps_reseting = 0
    prev = 0
    memory_threshold = 1500  # 3.5GB
    temp_outlets = []
    gridcells_dqn = []

    def __init__(self, period: str):
        Period(period)
        self.polygon = traci.polygon
        self.route = traci.route
        self.vehicle = traci.vehicle
        self.poi = traci.poi
        self.gui = traci.gui
        self.simulation = traci.simulation

    def get_polygons(self):
        all_polygon_ = self.polygon.getIDList()
        return all_polygon_

    def get_buildings(self):
        all_builds_ = []
        for id_poly in self.get_polygons():
            if self.polygon.getType(id_poly) == "building":
                all_builds_.append(id_poly)
        return all_builds_

    def prepare_route(self):
        """
        add routes to env_variables
        where the routes generated by randomTrips and store in random_routes_path
        """
        tree = ET.parse(env_variables.random_routes_path)
        root = tree.getroot()
        for child_root in root:
            id_ = child_root.attrib["id"]
            for child in child_root:
                # print(child.tag, child.attrib)
                # if child_root.tag == 'route':
                edges_ = list((child.attrib["edges"]).split(" "))
                # print('the id: {}  , edges: {}'.format(id_, edges_))
                self.route.add(id_, edges_)
                env_variables.all_routes.append(id_)
        del tree
        del root

    def update_outlet_color(self, id_, value):
        color_mapping = {
            (9, 10): (64, 64, 64, 255),  # dark grey
            (6, 9): (255, 0, 0, 255),  # red
            (3, 6): (0, 255, 0, 255),  # green
            (1, 3): (255, 255, 0, 255),  # yellow
        }

        for val_range, color in color_mapping.items():
            if value >= val_range[0] and value <= val_range[1]:
                traci.poi.setColor(id_, color)
        del color_mapping

    def get_all_outlets(self):
        """
        get all outlets and add id with position to env variables
        """
        outlets = []
        poi_ids = traci.poi.getIDList()

        def append_outlets(id_):
            type_poi = traci.poi.getType(id_)

            if type_poi in env_variables.types_outlets:
                position_ = traci.poi.getPosition(id_)
                env_variables.outlets[type_poi].append((id_, position_))
                val = 0
                if type_poi == "3G":
                    val = 850
                elif type_poi == "4G":
                    val = 1250
                elif type_poi == "5G":
                    val = 10000
                elif type_poi == "wifi":
                    val = 550
                factory = FactoryCellular(
                    outlet_types[str(type_poi)],
                    1,
                    1,
                    [1, 1, 0],
                    id_,
                    [position_[0], position_[1]],
                    10000,
                    [],
                    [10, 10, 10],
                )

                outlet = factory.produce_cellular_outlet(str(type_poi))
                outlet.outlet_id = id_
                outlet.radius = val
                outlets.append(outlet)

        list(map(lambda x: append_outlets(x), poi_ids))

        # satellite = Satellite(1, [1, 1, 0], 0, [0, 0],
        #                       100000, [],
        #                       [10, 10, 10])
        # outlets.append(satellite)

        del poi_ids

        return outlets

    def distance(self, outlet1, outlet2):
        """Returns the Euclidean distance between two outlets"""
        return math.sqrt(
            (outlet1.position[0] - outlet2.position[0]) ** 2
            + (outlet1.position[1] - outlet2.position[1]) ** 2
        )

    def fill_grids_with_the_nearest(self, outlets):
        sub_dis = []
        for j in outlets:
            dis = []
            for i in outlets:
                dis.append(self.distance(j, i))
            if len(dis) >= 3:
                sorted_dis = sorted(dis)
                min_indices = [dis.index(sorted_dis[i]) for i in range(3)]
                elements = [outlets[i] for i in min_indices]
                outlets = [
                    element
                    for index, element in enumerate(outlets)
                    if index not in min_indices
                ]
                sub_dis.append(elements)
        return sub_dis

    @staticmethod
    def fill_grids(grids):
        Grids = {
            "grid1": [],
            "grid2": [],
            "grid3": [],
            "grid4": [],
            "grid5": [],
            "grid6": [],
            "grid7": [],
        }

        def grid_namer(i, grid):
            name = "grid" + str(i + 1)
            Grids[name] = grid

        list(map(lambda x: grid_namer(x[0], x[1]), enumerate(grids)))
        return Grids

    def select_outlets_to_show_in_gui(self):
        """
        select outlets in .network to display type of each outlet
        """
        # for key in env_variables.outlets.keys():
        #     for _id,_ in env_variables.outlets[key]:
        #         self.gui.toggleSelection(_id, 'poi')
        from itertools import chain

        array = list(
            map(
                lambda x: x,
                chain(*list(map(lambda x: x[1], env_variables.outlets.items()))),
            )
        )
        list(
            map(
                lambda x: self.gui.toggleSelection(x[0], "poi"), map(lambda x: x, array)
            )
        )
        del array

    def get_positions_of_outlets(self, outlets):
        positions_of_outlets = []

        list(map(lambda x: positions_of_outlets.append(x.position), outlets))
        return positions_of_outlets

    def generate_vehicles(self, number_vehicles):
        """
        It generates vehicles and adds it to the simulation
        and get random route for each vehicle from routes in env_variables.py
        :param number_vehicles: number of vehicles to be generated
        """

        all_routes = env_variables.all_routes

        def add_vehicle(id_route_):
            uid = str(uuid4())
            self.vehicle.add(vehID=uid, routeID=id_route_)

            env_variables.vehicles[uid] = Car(uid, 0.0, 0.0)

        list(map(add_vehicle, ra.choices(all_routes, k=number_vehicles)))
        del all_routes

    def starting(self):
        """
        The function starts the simulation by calling the sumoBinary, which is the sumo-gui or sumo
        depending on the nogui option
        """

        os.environ["SUMO_NUM_THREADS"] = "8"
        # show gui
        # sumo_cmd = ["sumo-gui", "-c", env_variables.network_path]
        # dont show gui
        sumo_cmd = ["sumo", "-c", env_variables.network_path]
        traci.start(sumo_cmd)

        # end the simulation and d

        self.prepare_route()

    def remove_vehicles_arrived(self):
        """
        Remove vehicles which removed from the road network ((have reached their destination) in this time step
        the add to env_variables.vehicles (dictionary)
        """
        ids_arrived = self.simulation.getArrivedIDList()

        def remove_vehicle(id_):
            # print("del car object ")
            del env_variables.vehicles[id_]

        if len(ids_arrived) != 0:
            list(map(remove_vehicle, ids_arrived))

    def add_new_vehicles(self):
        """
        Add vehicles which inserted into the road network in this time step.
        the add to env_variables.vehicles (dictionary)
        """
        ids_new_vehicles = traci.vehicle.getIDList()

        def create_vehicle(id_):
            env_variables.vehicles[id_] = Car(id_, 0, 0)

        list(map(create_vehicle, ids_new_vehicles))

    def run(self):
        self.starting()
        outlets = self.get_all_outlets()
        self.Grids = self.fill_grids(self.fill_grids_with_the_nearest(outlets[:21]))
        step = 0
        print("\n")
        outlets_pos = self.get_positions_of_outlets(outlets)
        observer = ConcreteObserver(outlets_pos, outlets)
        performance_logger = PerformanceLogger()
        # set the maximum amount of memory that the garbage collector is allowed to use to 1 GB
        max_size = 273741824

        gc.set_threshold(700, max_size // gc.get_threshold()[1])
        gc.collect(0)
        build = []
        for i in range(1):
            build.append(RLBuilder())
            self.gridcells_dqn.append(
                build[i]
                .agent.build_agent(ActionAssignment())
                .environment.build_env(CentralizedReward(), CentralizedState())
                .model_.build_model("centralized", 12, 2)
                .build()
            )
            print(" path6  : ", path6)
            self.gridcells_dqn[i].model.load_weights(
                os.path.join(prev_centralize_weights_path, f"weights_{i}.hdf5")
            )
            # self.gridcells_dqn[i].agents.fill_memory(self.gridcells_dqn[i].agents.memory , os.path.join(prev_centralize_memory_path, f'centralize_buffer.pkl'))
            self.gridcells_dqn[i].agents.grid_outlets = self.Grids.get(f"grid{i + 1}")
            self.gridcells_dqn[i].agents.outlets_id = list(
                range(len(self.gridcells_dqn[i].agents.grid_outlets))
            )

        for i in range(1):
            for index, outlet in enumerate(self.gridcells_dqn[i].agents.grid_outlets):
                # outlet.dqn.model.load_weights(os.path.join(prev_decentralize_weights_path, f'weights_{index}.hdf5'))
                # outlet.dqn.agents.fill_memory(outlet.dqn.agents.memory , os.path.join(prev_decentralize_memory_path, f'decentralize_buffer{index}.pkl'))
                self.temp_outlets.append(outlet)
                # print("outlet : ", outlet.__class__.__name__)

        while step < env_variables.TIME:
            # print("env_variables.vehicles : ",env_variables.vehicles)
            # print("............................... : ", performance_logger.handled_services)

            process = psutil.Process()
            memory_usage = process.memory_info().rss / 1024.0 / 1024.0  # Convert to MB
            # print(f"Memory usage at step {step}: {memory_usage:.2f} MB")
            if memory_usage > self.memory_threshold:
                gc.collect(0)

            gc.collect(0)
            self.steps += 1
            traci.simulationStep()
            if step == 0:
                number_cars = int(
                    nump_rand.normal(
                        loc=env_variables.number_cars_mean_std["mean"],
                        scale=env_variables.number_cars_mean_std["std"],
                    )
                )
                self.generate_vehicles(number_cars)

            if traci.vehicle.getIDCount() <= env_variables.threashold_number_veh:
                number_cars = int(
                    nump_rand.normal(
                        loc=env_variables.number_cars_mean_std["mean"],
                        scale=env_variables.number_cars_mean_std["std"],
                    )
                )
                self.generate_vehicles(number_cars)

            self.remove_vehicles_arrived()
            print("step is ....................................... ", step)

            # if self.steps - previous_steps_sending == frame_rate_for_sending_requests:
            #     previous_steps_sending = self.steps
            number_of_cars_will_send_requests = round(
                len(list(env_variables.vehicles.values())) * 0.1
            )
            vehicles = ra.sample(
                list(env_variables.vehicles.values()), number_of_cars_will_send_requests
            )
            list(
                map(
                    lambda veh: requests_buffering(
                        veh, observer, performance_logger, self.steps, self.previous_period
                    ),
                    vehicles,
                )
            )
            if self.steps == 2:
                centralize_state_action(self.gridcells_dqn, self.steps, performance_logger)

            if self.steps - self.previous_period >= 10:
                self.previous_period = self.steps
                decentralize_period(performance_logger, self.gridcells_dqn, step)
                for i, outlet in enumerate(self.gridcells_dqn[0].agents.grid_outlets):
                    performance_logger.outlet_services_ensured_number_with_action_period[
                        outlet
                    ] = [
                        0,
                        0,
                        0,
                    ]
                    performance_logger.outlet_services_requested_number_with_action_period[
                        outlet
                    ] = [
                        0,
                        0,
                        0,
                    ]
                    performance_logger.outlet_services_power_allocation_with_action_period[
                        outlet
                    ] = [
                        0,
                        0,
                        0,
                    ]
                    # performance_logger.outlet_services_power_allocation_without_accumilated_with_action_period[
                    #     outlet] = []
                list(
                    map(
                        lambda veh: terminate_service(veh, outlets, performance_logger),
                        env_variables.vehicles.values(),
                    )
                )

            if self.steps - self.previous_steps_centralize_action >= 40:
                self.previous_steps_centralize_action = self.steps
                centralize_nextstate_reward(self.gridcells_dqn)
                centralize_state_action(self.gridcells_dqn, step, performance_logger)

            if self.steps - self.prev == self.snapshot_time:
                self.prev = self.steps
                take_snapshot_figures()

            else:
                close_figures()

            if self.steps - self.previous_steps >= env_variables.decentralized_replay_buffer:
                self.previous_steps = self.steps
                qvalue = []
                for ind, gridcell_dqn in enumerate(self.gridcells_dqn):
                    for i, outlet in enumerate(gridcell_dqn.agents.grid_outlets):
                        if len(outlet.dqn.agents.memory) > 31:
                            # print("replay buffer of decentralize ")
                            outlet.qvalue = (
                                outlet.dqn.agents.replay_buffer_decentralize(
                                    30, outlet.dqn.model
                                )
                            )
                            qvalue.append(outlet.qvalue)

            # if self.steps - previous_steps_centralize >= env_variables.centralized_replay_buffer:
            #     previous_steps_centralize = self.steps
            #     for ind, gridcell_dqn in enumerate(self.gridcells_dqn):
            #         if len(gridcell_dqn.agents.memory) >= 64:
            #             # print("replay buffer of centralize ")
            #             average_qvalue_centralize.append(gridcell_dqn.agents.replay_buffer_centralize(32,
            #                                                                                           gridcell_dqn.model))

            if self.steps - self.previouse_steps_reseting >= env_variables.episode_steps:
                # print("reset the environment : ")
                self.previouse_steps_reseting = self.steps
                list_ = []
                # avg_qvalue = (sum(average_qvalue_centralize) / len(average_qvalue_centralize))
                # print("avg_qvalue : ", avg_qvalue)
                for ind, gridcell_dqn in enumerate(self.gridcells_dqn):
                    # gridcell_dqn.environment.reward.gridcell_reward_episode = sum(
                    #     gridcell_dqn.environment.reward.reward_value)
                    # update_lines_reward_centralized(lines_out_reward_centralize, self.steps, self.gridcells_dqn)
                    # update_lines_Qvalue_centralized(lines_out_Qvalue_centralize, self.steps, avg_qvalue
                    #                                 )

                    # self.add_value_to_pickle(
                    #         os.path.join(centralize_qvalue_path, f'qvalue.pkl'),
                    #         avg_qvalue)

                    update_figures(self.steps, self.temp_outlets)

                    for i, out in enumerate(gridcell_dqn.agents.grid_outlets):
                        add_value_to_pickle(
                            os.path.join(decentralize_qvalue_path, f"qvalue{i}.pkl"),
                            out.qvalue,
                        )
                        out.dqn.environment.reward.episode_reward_decentralize = (
                            out.dqn.environment.reward.reward_value
                        )
                        # print("out.qvalue  : ", out.qvalue)
                        # self.add_value_to_text(
                        #     f"H://work_projects//network_slicing//ns//results//qvalue{i}.txt",
                        #     out.qvalue)

                        # out.dqn.environment.state.resetsate(out._max_capacity)
                        out.dqn.environment.reward.reward_value = 0
                        out.dqn.environment.state.mean_power_allocated_requests = 0.0
                        # out.dqn.environment.state.state_value_decentralize = out.dqn.environment.state.calculate_state()
                        out.dqn.environment.state.state_value_decentralize = [
                            [0.0] * 9 for _ in range(3)
                        ]
                        out.current_capacity = out._max_capacity
                        # print("the max : ", out._max_capacity)
                        # print("reset capacity : ", out.current_capacity)
                        list_.append(out._max_capacity)

                    gridcell_dqn.environment.reward.resetreward()
                    gridcell_dqn.environment.state.resetsate(list_)
                    gridcell_dqn.environment.reward.reward_value = 0
                    states = []
                    for j, outlet in enumerate(gridcell_dqn.agents.grid_outlets):
                        for i in range(3):
                            gridcell_dqn.environment.state.index_outlet = j
                            gridcell_dqn.environment.state.index_service = i
                            state = gridcell_dqn.environment.state.calculate_state()
                            states.append(state)
                    gridcell_dqn.environment.state.state_value_centralize = states
                performance_logger.reset_state_decentralize_requirement()
                average_qvalue_centralize = []

            step += 1

            if step == env_variables.TIME:
                self.save()

        self.close()

    def close(self):
        plt.close()
        traci.close()

    def save(self):
        for index, g in enumerate(self.temp_outlets):
            g.dqn.model.save_weights(os.path.join(path7, f"weights_{index}.hdf5"))
            g.dqn.agents.free_up_memory(
                g.dqn.agents.memory,
                os.path.join(
                    path_memory_decentralize, f"decentralize_buffer{index}.pkl"
                ),
            )
            # for i in range(1):
            #     print()
            # self.gridcells_dqn[i].model.save_weights(os.path.join(path6, f'weights_{i}.hdf5'))
            # self.gridcells_dqn[i].agents.free_up_memory(self.gridcells_dqn[i].agents.memory , os.path.join(path_memory_centralize, f'centralize_buffer.pkl'))

            # shutil.copytree(results_dir, prev_results_dir)
